---
title: "Unit 12 | Multivariate Time Series"
output: html_notebook
---

```{r library-imports}

library(tidyverse)
library(tswge)
library(kableExtra)
library(tseries)
library(orcutt)
library(vars)
library(RColorBrewer)

```


# For Live Session

## Activity 1


Activity 1: On five to six PowerPoint slides, address the question of interest. 

The attached data set (LA_Cmort_Study.csv) is a portion of the data taken from a study (Shumway 1988) on the possible effects of pollution and temperature on weekly cardiac mortality (heart attacks) in Los Angeles County. Your goal is to utilize all given information to provide the most useful forecasts for the next 52 weeks of cardiac mortality. 

You should include plots, tables, and charts to help make your analysis and inferences clear to your peers.


## Activity 2


Activity 2: On three to four Power Point slides, find the ASE for forecasts of the last five observations of the sales data set example for both the MLE model (using ARIMA) and the VAR model we fit. We fit several models using the arima() function, you should pick the one you feel is the best model (maybe with the lowest AIC?).

Your slides should include each of the following:

- Your code
- Enough visual aids to describe how you calculated the ASE for each model
- The ASE for each model
- Which model you feel is better in that respect, and why. 



# Asynchronous Material

## 12.1 Introduction

## 12.2 Multivariate Regression with Correlated Errors, Part I

### 12.2.1 Setting and Notation

- univariate (one-variable)
- multivariate (more than one time series variable)


$$\begin{equation}
   Y_t = B_0 + B_1X_{t1} + B_2X_{t2} + ... + B_mX_{tm} + Z
\end{equation}$$


- Each realization is of length n in the equation above.

Response

- Y_t = Y_1, Y_2, ...Y_n

Noise

- Z_t = Z_1, Z_2, ...Z


Independent Variables

X_t1 = X_11, X_21, ..., X_n1


*m* is the number of variables that we have


_Note_ that for the independent variables.

- The first subscript is time (1, 2, ..., n)

- The second subscript indicates which variable it is


### 12.2.2 R-Based Analysis

_Step 1:_  Perform a regression analysis and model the residuals.  The following is example code (with m=3).

_Step 2:_ Use function ARIMA to perform the MLE analysis which estimates the coefficients in the multiple regression while simultaneously modeling the Zt as an AR(phi$p)


*fit$coef* contains the AR coefficients, the constant, and the coefficients on x1, x2, and x3

Recall that we don't usually look at the SE's for AR and MA coefficients (the factor tables gives more information).

Function *arima* doesn't give p-values, but in general if the absolute values of the coefficient is over *two times* the SE, this is evidence at the .05 level that the variable is useful.

The final model residuals are given in *fit$resid*, and they should be white.
  - Check with residual plots and/ or Ljung-Box Test

```{r 12.2.2}

# Step 1
# ksfit = lm(y ~ x1 + x2 + x3)
# phi = aic.wge(ksfit$residuals, p=0:8)

# Step 2
# fit = arima(sales, order = c(phi$p, 0, 0), xreg = cbind(x1, x2, x3))


```



### 12.2.3 Sales Data Example


### 12.2.4 Lagged Variables



Creating Lagged Variables in R

```{r 12.2.4}

df = data.frame(Y = c(1, 1, 2, 3, 4, 4, 5, 8), X1 = c(5, 6, 6, 7, 7, 8, 8, 9))

# df

X1_L1 = numeric(length(df$X1))
X1_L1 = c(NA, df$X1[1:length(df$X1)-1]) # X1 lagged once
X1_L2 = c(NA, NA, df$X1[(1:(length(df$X1)-2))]) # X1 lagged twice

df$X1_L1 = X1_L1
df$X1_L2 = X1_L2

df

```


```{r 12.2.5}

df_business_sales = read.csv("./data/businesssales.csv")

ad_tv1 = c(NA, df_business_sales$ad_tv[1:length(df_business_sales$ad_tv)-1]) # ad_tv lagged once
ad_online1 = c(NA, df_business_sales$ad_online[1:length(df_business_sales$ad_online)-1]) # ad_online lagged once

df_business_sales$ad_tv1 = ad_tv1
df_business_sales$ad_online1 = ad_online1

df_business_sales


```

## 12.3 Multivariate Regression with Correlated Errors, Part II

### 12.3.1

```{r 12.3.1.1}


df_business_sales = read.csv("./data/businesssales.csv")

ad_tv1 = dplyr::lag(df_business_sales$ad_tv, 1)
ad_online1 = dplyr::lag(df_business_sales$ad_online, 1)

discount = df_business_sales$discount

df_business_sales$ad_tv1 = ad_tv1
df_business_sales$ad_online1 = ad_online1


ksfit = lm(sales~ ad_tv1 + ad_online1 + discount, data = df_business_sales)
aic.wge(ksfit$residuals, p=0:8, q=0:0) # AIC picks p = 7
fit = arima(df_business_sales$sales, order = c(7,0,0), xreg = cbind(ad_tv1, ad_online1, discount))
fit

```

```{r 12.3.1.2}


t = 1:100

ksfit = lm(sales ~ t + ad_tv1 + ad_online1 + discount, data = df_business_sales)
aic.wge(ksfit$residuals, p=0:8, q=0:0) # AIC picks p = 7
fit = arima(df_business_sales$sales, order = c(7,0,0), xreg = cbind(t, ad_tv1, ad_online1, discount))
fit

```

### 12.3.2 Concept Check

Which answer below best describes our strategy with multiple regression with correlated errors?

_We fit a multiple regression model to the data as we usually would, assuming the errors are not serially correlated. This includes any lagged variables we suspect are present. We then model the residuals from that model with an AR or ARMA process and proceed to find the MLE estimates from an ARIMA fit._


### 12.3.3 Cross-Correlation

A useful tool for detecting the existence of "lagged relationships" in multivariate time series analysis is the cross-correlation function.


_The "cross-correlation" between variables X_t1 and X_t2 at lag k is the correlation between X_t,1 and  X_t+k,2._


```{r 12.2.4}

df_lag = read_csv("./data/whatisthelag.csv")


ccf(df_lag$Y, df_lag$X1)

```



## 12.4 Vector AR (VAR) Models


### 12.4.1


_Univariate_  X_t : X_1 , ....Xn (n is time)

_Multivariate_ X_t



$$\begin{equation}
   X_t = (1 - \phi_1)\mu + \phi X_{t-1} + a_t
\end{equation}$$

- univariate 
- 1 variable, X_t
- the model involves X_t and X_t-1

m variables

X_t1, X_t2, ...,  X_tm  (1st variable is time and the 2nd is whatever variable it is)

Recall that an AR(1): 



$$\begin{equation}
   X_{t1} = (1 - \phi_{11})\mu_1 - \phi_{12}\mu_2 + \phi_{11}X_{t-1,1} + \phi_{12}X_{t-1,2} + a_t
\end{equation}$$


Bivariate AR(1) [ VAR(1) ]

- 2 variables : X_t1, X_t2 (mean mu 1, mean mu 2) and a_t1, a_t2


### 12.4.2 VAR Forecasting


AR(p) forecasts


- forecasts depend on model and some of the observed values

$$\begin{equation}
   \hat{X_{t0}}(l) = \bar{X}(1 - \phi_{1} - \phi_{2}) + \phi_1{1}\hat{X}_{t0}(l - 1) + ..+ \phi_{p}\hat{X}_{t0}(l - p) 
\end{equation}$$





### 12.4.3 VAR Model Example 1

_VARselect_

_preds$fcst$x1[1:5,1]_



```{r 12.4.3}

x1.25 = c(-1.03, 0.11, -0.18, 0.20, -0.99, -1.63, 1.07, 2.26, -0.49, -1.54, 0.45, 0.92, -0.05, -1.18, 0.90, 1.17, 0.31, 1.19, 0.27, -0.09, 0.23, -1.91, 0.46, 3.61, -0.03)

x2.25 = c(-0.82, 0.54, 1.13, -0.24, -0.77, 0.22, 0.46, -0.03, -0.59, 0.45, 0.59, 0.15, 0.60, 0.13, -0.04, 0.12, -0.96, 0.23, 1.81, -0.01, -0.95, -0.55, -0.15, 0.71, 0.90)

x1 = x1.25[1:20]
x2 = x2.25[1:20]

p1 = aic.wge(x1, p=0:8, q=0:0)
# aic picks p2
x1.est = est.ar.wge(x1, p=p1$p)
fore.arma.wge(x1, phi=x1.est$phi, n.ahead = 5, lastn = FALSE, limits = FALSE)


p2 = aic.wge(x2, p=0:8, q=0:0)
# aic picks p2

x2.est = est.ar.wge(x2, p=p2$p)
fore.arma.wge(x2, phi=x2.est$phi, n.ahead = 5, lastn = FALSE, limits = FALSE)

# VAR and VARselect are from CRAN package vars
X = cbind(x1, x2)
VARselect(X, lag.max = 6, type = "const", season = NULL, exogen = NULL)
# VARselect picks p=5 (using AIC)
lsfit = VAR(X, p=5, type = "const")
lsfit

preds = predict(lsfit, n.ahead = 5)

preds

preds$fcst$x1[1:5,1]

```
### 12.4.4 Concept Check

```{r 12.4.4}

summary(lsfit)


```


### 12.4.5 VAR Model Example 

- This example was constructed to show ho one variable in a VAR model can be a leading indicator for another variable.

- Because variable X_t1 = 2X_t2 - 5,2, we would expect the VAR forecasts to somehow take advantage of this fact.

- Notice that
 - Both X_t1 and X_t2 were generated as AR(2) processes
 - AIC applied to each series separately picked an AR(2)
 - Function _VARselect_ identified the VAR model as a VAR(5)
  - It was necessary that p was at least 5 so that lag 5 would be in the fitted model.
  - AIC detected this "by itself".
  - The fitted VAR(5) MODEL GAVE ESSENTIALLY PERFECT FORECASTS FOR X_t1 for steps ahead up to 5.
  - The forecasts for X_t2 showed no detectable improvement over the univariate forecasts.
  
  
```{r 12.4.5}

# library(RColorBrewer)

fanchart(preds, colors = brewer.pal(n = 8, name = "Blues")) # change the color palette

```




### 12.4.6 Concept Check


We found that the individual data sets, Xt1 and Xt2, both had errors that followed an AR(2) process. Why then was the VAR(5) selected to be the most appropriate model?

_One of the variables was very associated with the other variable at a lag of 5._


## 12.5 Melanoma and Sunspot Example

### 12.5.1 Annual DAta from 1936 to 1972


```{r 12.5.1.1}

melenoma = c(1.0, 0.9, 0.8, 1.4, 1.2, 1.0, 1.5, 1.9, 1.5, 1.5, 1.5, 1.6, 1.8, 2.8, 2.5, 2.5, 2.4, 2.1, 1.9, 2.4, 2.4, 2.6, 2.6, 4.4, 4.2, 3.8, 3.4, 3.6, 4.1, 3.7, 4.2, 4.1, 4.1, 4.0, 5.2, 5.3, 5.3)

sunspot = c(40, 115, 100, 80, 60, 40, 23, 10, 10, 25, 75, 145, 130, 130, 80, 65, 20, 10, 5, 10, 60, 190, 180, 175, 120, 50, 35, 20, 10, 15, 30, 60, 105, 105, 105, 80, 65)


mel.67 = melenoma[1:32]
sun.67 = sunspot[1:32]

p.mel = aic.wge(x1, p=0:8, q=0:0)
p.mel

# aic picks ??
mel.est = est.ar.wge(mel.67, p=p.mel$p)
fore.arma.wge(mel.67, phi=mel.est$phi, n.ahead = 5, lastn = FALSE, limits = FALSE)


p.sun = aic.wge(sun.67, p=0:8, q=0:0)
p.sun
# aic picks p3

sun.est = est.ar.wge(x2, p=p.sun$p)
fore.arma.wge(sun.67, phi=x2.est$phi, n.ahead = 5, lastn = FALSE, limits = FALSE)

# VAR and VARselect are from CRAN package vars
X = cbind(mel.67, sun.67)
VARselect(X, lag.max = 6, type = "const", season = NULL, exogen = NULL) # AIC = 5.04
# VARselect picks p=5 (using AIC)
lsfit = VAR(X, p=4, type = "const")
lsfit

preds = predict(lsfit, n.ahead = 5)

preds
# 

```


```{r 12.5.1.2}

ccf(sunspot, melenoma, ylim = c(-1,1))

```

```{r 12.5.1.3}

fore.arma.wge(mel.67, phi=mel.est$phi, n.ahead = 5, lastn = FALSE, limits = FALSE)

```

```{r 12.5.1.4}

fore.arma.wge(sun.67, phi=x2.est$phi, n.ahead = 5, lastn = FALSE, limits = FALSE)

```

```{r 12.5.1.5}

plot(seq(1,37,1), melenoma, type="b", ylim=c(0,6))
points(seq(33,37,1), preds$fcst$mel.67[1:5, 1], type="b", pch = 15)
fanchart(preds)

```

### 12.5.2 VARSelect

Business Sales Data with VAR Model

What is the order estimated by VARSelect? 

Use the BIC, which is noted as “SC(n)." `2`

```{r 12.5.2}

BSales2 = read.csv("./data/businesssales.csv")

sales = BSales2$sales 
at_tv = BSales2$ad_tv
ad_online = BSales2$ad_online
discount = BSales2$discount

X = cbind(at_tv, ad_online, discount)

VARselect(X, lag.max = 6, type = "const", season = NULL, exogen = NULL) # AIC = 5.04


```

### 12.5.2 VARSelect

Step 3: Use VAR() and the order estimated in the last step to fit the VAR model. Fit it with a trend and without a trend.

Is there enough evidence to suggest the trend term is appropriate?  `No`

Question:  Is this because most of the values are very close to zero? (for example, `0.001800787`)

```{r 12.5.3}



lsfit = VAR(X, p=2, type = "both",  season = NULL, exogen = NULL)
lsfit


```

```{r 12.5.4}

ccf(sales, at_tv, ylim = c(-1,1))

```


```{r 12.5.5}

VARselect(X, lag.max = 6, type = "none", season = NULL, exogen = NULL) # AIC = 5.04


# aic5.wge(lsfit$residuals, p=0:8, q=0:0) # AIC picks p = 7
# fit = arima(df_business_sales$sales, order = c(2,0,0), xreg = cbind(ad_tv1))
# fit

```








