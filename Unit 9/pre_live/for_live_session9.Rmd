---
title: "Unit 9 - Model Identification for Stationary Models"
output: html_notebook
---

# For Live Session

```{r library-imports}

library(tidyverse)
library(tswge)

```



## Question 1 

In preparation for the next live session, complete the following. Be sure and submit your work to the "Unit 9: "For Live Session" Assignment" assignment on 2DS:

Please address each activity on at least one PowerPoint slide and submit via the online campus. 

Using the Texas gas price data (`TexasGasPrices.csv`), we found that an AR(2) was the model suggested by both the AIC and BIC. We would now like to compare the fit from using the maximum likelihood and Burg estimates. 

Provide as much detail as you need to adequately describe the question of interest. For example

- Provide detail and context, in addition to a number, for questions asking for an estimate.
- You may/should provide code (because your audience is your peers).
- You should provide adequate labels.
- Any other pertinent details to sufficiently convey your response.


Using at least one slide per question

- Fit an AR(2) to the data using the maximum likelihood estimates like you did in the Concept Check question.
- Fit and AR(2) to the data using the Burg estimates.  Display and describe.
- Find the ASE for the maximum likelihood fit by forecasting the last 24 weeks of the series.
- Find the ASE for the Burg fit by forecasting the last 24 weeks of the series.
- Which model would you choose?



# Asynchronous Material

## 9.1 Introduction

### Introduction and Setting


**Model Free Methods**

- Windowed spectral estimators
- Sample autocorrelations
- Filtering procedures


**Model Dependent Techniques**

- AR models
- Signal-plus-noise models
- ARMA models
- ARIMA models
- Seasonal models

## 9.2 Estimation: Introduction

### Estimation Methods for Stationary Models

  `est.arma.wge`
    - Maximum likelihood estimation only
  `est.ar.wge`
    - Maximum likelihood
    - Yule-Walker
    - Burger
    
    
- For ARMA(p,q) models with q > 0, tswge only produces maximum likelihood estimates.

- For AR(p) models, tswge provides ML, YW. and Burg estimates



    
## 9.3 Estimation: Maximum Likelihood Estimation

### 9.3.1  

- Involves maximizing the likelihood function L = f(x_1, x_2, ..., x_n), which is the joint distribution of the time series realization

- Iterative, computationally intensive procedure

- Involves distributional assumptions about white noise (we will assume normal white noise)

- Is applicable to AR(p), MA(q) and ARMA(p,q) models


### 9.3.2 tswge

The key function for ML estimation in an ARMA model is `est.arma.wge`(data, p, q)

Type:  

- mle (only)

However, if you know that the model is an AR(p) model, then you can use

`est.ar.wge`(data, p, q, type)

Types:

- mle (default)
- yw
- burg


Final Model Estimation

$$\begin{equation}
   (1 - 1.64B + .81B^2)(X_t - 49.96) = (1 - .87B)a_t ;  \sigma^2_a = 5.19
\end{equation}$$

```{r 9.3.2.1}

# ARMA(2,1)

# (1 - 1.6B + .8B^2)(X_t - 50) = (1 - .8B)a_t, sigma_squared = 5

x21 = gen.arma.wge(n=100, phi = c(1.6, -.8), theta = .8, vara = 5, sn = 55, plot = FALSE)
x21 = x21 + 50 # gen.arma generates data from a zero mean model.  
plot.ts(x21)
# We use this strategy to generate the AR(2) model with mean 50.
est.arma.wge(x21, p=2, q=1)
mean(x21)


```

Final Model Estimation

$$\begin{equation}
   (1 - .06B + .69B^2 - .11B^3 + .68B^4)(X_t - 19.98) = a_t ;  \sigma^2_a = 8.78
\end{equation}$$



```{r 9.3.2.2}

# ARMA(2,1)

# (1 + .7B^2 - .1B^3 + .72B^4)(X_t - 20) = a_t, sigma_squared = 10

x21 = gen.arma.wge(n=100, phi = c(0, -.7, .1, -.72), vara = 4, sn = 72, plot = FALSE)
x21 = x21 + 20 # gen.arma generates data from a zero mean model.  
plot.ts(x21)
# We use this strategy to generate the AR(2) model with mean 50.
est.arma.wge(x21, p=4, q=1)
mean(x21)


```


```{r 9.3.3}

# ARMA(2,1)

# (1 - .3B + .7B^2)(X_t - 37) = (1 + .4B)a_t, sigma_squared = 4

x21 = gen.arma.wge(n=200, phi = c(.3, -.7), theta = -.4, vara = 4, sn = 27, plot = FALSE)
x21 = x21 + 37 # gen.arma generates data from a zero mean model.  
plot.ts(x21)
# We use this strategy to generate the AR(2) model with mean 50.
est.arma.wge(x21, p=2, q=1)
mean(x21)



```
    
## 9.4 Estimation: Yule–Walker Estimates

Change p_k to p_k (hat) and solve the resulting system of equations for phi_1_hat, ..., phi_p_hat


These are the Yule-Walker estimates


## 9.5 Estimation: Burg Estimates

In AR models, there is no preference in time direction (recall p_k = p_-k)

Goal is to minimize forward-backward least squares.

Burg showed how to miminize the forward-backward least squares, while at the same time assuring a stationary solution.

### 9.4.4

- Maximum likelihood estimation

_Estimates found by finding the values that provide the largest value of the joint distribution of the time series realization. This is an iterative procedure._

- Yule–Walker estimation

_Estimates found by solving a system of linear equations by estimating the autocorrelations._

- Burg estimation

_Estimates found by minimizing the forward backward least squares._

Match the estimation models to the correct description.

### 9.5.5   

Which estimates can estimate parameters from both stationary and nonstationary models?

_Maximum likelihood estimates_


### 9.5.6

```{r 9.5.6}

# Generate Data from an AR(2):  (1 - 1.6B + .9B^2)X_t = a_t

x = gen.arma.wge(n = 200, phi = c(1.6, -.9), vara = 37, sn = 33)
plotts.wge(x)

# Yule-Walker Estimates
x.yw = est.ar.wge(x, p=2, type = "yw")
x.yw

# Burg Estimates
x.burg = est.ar.wge(x, p=2, type="burg")
x.burg


# Maximum Likelihood Estimates
x.mle = est.ar.wge(x, p=2, type="mle")
x.mle

```


### 9.5.7

```{r 9.5.7}

# Generate Data from an AR(2):  (1 - .3B + .7B^2)(X_t - 37) = (1 + .4B)a_t  ; sigma_2 = 4

x = gen.arma.wge(n = 200, phi = c(.3, -.7), theta = -.4, vara = 4, sn = 33, plot = FALSE)
x = x + 37
plotts.wge(x)

# Maximum Likelihood Estimates
x.mle= est.arma.wge(x, p=2, q = 1)
x.mle


```

### 9.5.8

```{r 9.5.8}

# Generate Data from an AR(2):  (1 - .3B + .7B^2)(X_t - 37) = a_t  ; sigma_2 = 4

x = gen.arma.wge(n = 200, phi = c(.3, -.7), vara = 4, sn = 33, plot = FALSE)
x = x + 37
plotts.wge(x)

# Burg Estimates
x.burg = est.ar.wge(x, p=2, type = "burg")
x.burg


```

## 9.6 Estimation: When It Makes a Difference

### 9.6.1

When the real roots are close to the unit circle, the Yule-Walker method does not perform well.

### 9.6.2

Models should not be used when roots are found to be inside the unit circle, this would indicate a non-stationary model.

Burg and Yule-Walker methods will always produce a stationary model.    

### 9.6.3

Which of the following statements is `FALSE`?

_Burg estimates may find a nonstationary model (produce roots inside the unit circle)._

## 9.7 Estimation: Noise Variance

```{r 9.7.1}

# Generate Data from an AR(2):  (1 - .3B + .7B^2)(X_t - 37) = a_t  ; sigma_2 = 4

x = gen.arma.wge(n = 100, phi = c(2.195, -1.994, .696), vara = 1, sn = 53, plot = TRUE)
plotts.wge(x)

x.mle = est.ar.wge(x, p=3, type = "mle")
x.mle$avar

mean(x.mle$res^2)


```

## 9.8 Model Identification: Introduction

Lesson:  Always plot the data.

- Check for white noise in the time series plot and via the ACF.

### 9.8.3


```{r 9.8.3}

df_white_noise = read.csv("./maybewhitenoise1.csv")
# head(df_white_noise)
plotts.wge(df_white_noise$x)
acf(df_white_noise$x)

```
### 9.8.4
```{r 9.8.4}

df_white_noise = read.csv("./maybewhitenoise2.csv")
# head(df_white_noise)
plotts.wge(df_white_noise$x)
acf(df_white_noise$x)

```

## 9.9 Model Identification: AIC Type, Part I


### 9.9.1

- Method 1 (AIC)
- Method 2 (Box-Jenkins)

Akaike's Information Criterion (AIC)

A general criterion for statistical model identification you have seen before in other settings.

AIC is actually one of a number of information-based criterion.

AICC
BIC
Others

### 9.9.2

Adding variables to an existing equation will always reduce the residual/ unexplained variability

In ARMA modeling, the unexplained variability is measured by sigma squared (a)

### 9.9.3 AIC in ARMA modeling

AIC:  imposes a penalty for adding terms

$$\begin{equation}
   AIC = ln(\hat\sigma^2_a) + 2(p + q + 1) / n
\end{equation}$$

- AIC has a tendency to select a higher-order model as the realization length increases.

- BIC imposes a stronger penalty for increasing p and q.

  - It tends to pick models with fewer parameters.
  - It avoids the problem of "over-modeling" large data sets.


- AICC has a penatly between that for AIC and BIC

- AIC and its variations are for selecting a stationary ARMA model.



```{r 9.9.4.1}

# Generate Data from an ARMA(1,1):  (1 - .967B)(X_t - 2.20) = (1 + .477B)a_t  ; sigma_2 = .0139

# Answer -4.20

x = gen.arma.wge(n = 100, phi = c(.967), theta = -.477, vara = .0139, sn = 53, plot = FALSE)
x = x + 2.20
plotts.wge(x)

x.mle = est.arma.wge(x, p=1, q=1)
x.mle$avar

# mean(x.mle$res^2)

aic.wge(x, p=1, q=1, type = "aic")


```


```{r 9.9.4.2}

# Generate Data from an ARMA(3, 0):  (1 - .1.45B + .734B^2 - .261B^3)(X_t - 2.20) = a_t  ; sigma_2 = .0137

# Answer -4.19


x = gen.arma.wge(n = 100, phi = c(1.45, -.734, .261), vara = .0137, sn = 53, plot = FALSE)
x = x + 2.20
plotts.wge(x)

x.mle = est.arma.wge(x, p=3, q=0)
x.mle$avar

# mean(x.mle$res^2)

aic.wge(x, p=3, q=0, type = "aic")


```

## 9.10 Model Identification: AIC Type, Part II


### 9.10.1 AIC in tswge

`aic.wge(x, p=0:5, q=0:2, type = "aic")`

### 9.10.2 AIC demo



```{r 9.10.2.1}

# Generate Data from an AR(3):  (1 - 2.555B + .2.42B^2 - .855B^3) = a_t  ; sigma_2 = 1
# factored form (1 - .95B)(1 - 1.6B + .9B^2)X_t =- a_t

# AIC Demo: aic.wge
# fig3.16a is a realization from the AR(3) model

data("fig3.16a")
# plotts.sample.wge provides a "look" at the data (which we always recommend)
plotts.sample.wge(fig3.16a)
# let tswge find the model with the lowest aic
aic.wge(fig3.16a, p=0:5, q=0:2, type="aic")
mean(fig3.16a)


```


```{r 9.10.2.2}

# Generate data from the AR(2,1)

x = gen.arma.wge(n=100, phi=c(1.6, -.9), theta=.8, sn=67, plot = FALSE)
x = x+10

plotts.sample.wge(x)
# no types listed below so it will use aic
aic.wge(x, p=0:8, q=0:4)
# picks ARMA(2,1)
est.arma.wge(x, p=2, q=1)
mean(x)


```

### 9.10.4

tswge and AIC5/ Top 5 Models

```{r 9.10.4.1}

x31 = gen.arma.wge(n = 75, phi = c(2.3, -1.92, .56), theta = -.8, sn=61, plot = FALSE)
x31 = x31 + 30
plotts.sample.wge(x31)

aic5.wge(x31, p=0:8, q=0:2)
# picks ARMA(6,1)
est.arma.wge(x31, p=6, q=1)

```

```{r 9.10.4.2}
# type BIC
aic5.wge(x31, p=0:8, q=0:2, type="bic")
# BIC picks ARMA(3,1)
est.arma.wge(x31, p=3, q=1)
mean(x31)
# mean is 25.73985
```
### 9.10.5

Because the AIC favored the AR(4,1) and the BIC favored the AR(2,1), which one should we pick?

_We should look at the ACF and spectral density to gather more information about the characteristics and features of the data. In addition we should look at the factor table for each model to help match the model with those characteristics. We will also need to use our intuition and any domain knowledge we may have to choose a model that makes the most sense and will be the most useful (because both of them are likely not the “correct” model)._


```{r 9.10.5}

df_inflation = read.csv("./inflation.csv")
# type AIC
aic5.wge(df_inflation$Inflation, p=0:8, q=0:2, type="aic")
# AIC picks ARMA(4,1)
# est.arma.wge(df_inflation$Inflation, p=4, q=1)
# mean(df_inflation$Inflation)
# mean is 25.73985


aic5.wge(df_inflation$Inflation, p=0:8, q=0:2, type="bic")
```


## 9.11 Model Identification: Box–Jenkins, Part I

### 9.11.4  

Box-Jenkins Model Identification for MA(q) and AR(p) models

MA(q) - look at the sample auto-correlation plot (ACF)

p_k = 0; for k > q

AR(p) - look at the sample partial auto-correlation plot (PACF)

phi_kk = 0; for k > p


## 9.12 Model Identification: Box–Jenkins, Part II

```{r 9.12.2}

df_arnawgatpq1 = read.csv("./armawhatpq1.csv")

acf(df_arnawgatpq1$x)

pacf(df_arnawgatpq1$x)

```

```{r 9.12.3}

# type AIC
aic5.wge(df_arnawgatpq1$x, p=0:8, q=0:2, type="aic")
# AIC picks ARMA(2,2)

# type BIC
aic5.wge(df_arnawgatpq1$x, p=0:8, q=0:2, type="bic")
# BIC picks ARMA(2,2)
```

```{r 9.12.4}

acf(df_inflation$Inflation)

pacf(df_inflation$Inflation)

```

## 9.13 Putting It All Together: Model ID, Estimation, Forecasting



```{r 9.13.4}


df_texasgasprice = read.csv("./texasgasprice.csv")

# head(df_texasgasprice)

plotts.sample.wge(df_texasgasprice$Price)

aic5.wge(df_texasgasprice$Price, type = "aic")
# picks AR(2)

aic5.wge(df_texasgasprice$Price, type = "bic")
# picks AR(2)

```



```{r 9.13.4.1}

est = est.ar.wge(df_texasgasprice$Price, p=2, type = "mle")
est

```

```{r 9.13.4.2}

tx_gas_forecast = fore.arma.wge(df_texasgasprice$Price, phi = est$phi, n.ahead = 8, plot = TRUE)

```

```{r 9.13.4.3}

tx_gas_forecast$f

# 2.087504 (1)
# 2.094299 (8)

```
